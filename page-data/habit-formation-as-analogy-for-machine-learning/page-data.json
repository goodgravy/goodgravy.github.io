{"componentChunkName":"component---src-templates-post-tsx","path":"/habit-formation-as-analogy-for-machine-learning/","result":{"data":{"site":{"siteMetadata":{"title":"jmsbrdy"}},"markdownRemark":{"id":"01807daf-e539-5855-a2e5-19b455881c67","excerpt":"When training an artificial neural network, a simplified version of the classic workflow is: Set up an neural network with randomly-weighted neurons Feed an example from the training set into the network Calculate the difference between the actual and expected output Use backpropagation to update…","html":"<p>When training an artificial neural network, a simplified version<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup> of the classic workflow is:</p>\n<ol>\n<li>Set up an neural network with randomly-weighted neurons</li>\n<li>Feed an example from the training set into the network</li>\n<li>Calculate the <a href=\"https://en.wikipedia.org/wiki/Loss_function\">difference between the actual and expected output</a></li>\n<li>Use <a href=\"https://en.wikipedia.org/wiki/Backpropagation\">backpropagation</a> to update the weights of all the neurons in the network</li>\n<li>Go to #2 until you have processed the entire training set</li>\n</ol>\n<p>A direct parallel can be drawn between this process and the habit formation described by Charles Duhigg in <a href=\"https://g.co/kgs/xm7DuX\">The Power of Habit</a>. He describes habitual behaviour as a three-step process<sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup>:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/39aa210b9e7cd621ef4442a0200f71ce/2bef9/3-step-habits.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 74.84662576687117%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABJzAAAScwGMIrkHAAADJElEQVQ4y41Sa0hTYRj+ptV2znFHN+fdzBI0BaFQokIJpDDsR9SPfpVEPzKT0iKSVou8EHndJJe3TDNNneiyzUuWczinuTR1zplLJ0lesDQqp5TE2zlnORPt8sHL+3znec7zfrzvi9AGZ2msEv3P+Wp++ndBUFAgspgesmj8xVTh/tmQlzHallK9oM+osxilDZbh/MoFgzjHYryXMmeq5zLFzVXI09NjvZm3lxeTF03F9nSG6bpLMJIOb5vjAczZsDyYBMvDmQAmESwNJAF8avehdZ8GC+zWmc3rGtFclwJ90chQR4mIEdwWxbkONKdq3jQngkERD701MTDbeQPmu6/DaKvoGcJOboGRaNSvV7PaentQ++veVcPFFyVrCjiSbBe6A3pVbvj8gNT48olIpyhJVMjz42rGtZLib1Mt/hTvFBCww3Xln+xqGdMqZFHeZTIA0LEpvbCUd/p0dBCFsX8NxY3PC54xGdiUdvMa4rP6cei7R6ldZnGs7n3aqaHJrDOD03kJ47P10hM0rxEetluYaWF626Hvi8qtrGySVlU9L1Uq9TK1uk+uaR9RarVnbYbL3fLIsexYmEw+DgbhUegXHgMoOAdzsrRbND+Vn2D/+FkTY9is013LLK+A5IJCEFdVQ2rRfShQKKGsseGBzRAsE4Sp6IZ2SBJvkidfULdmXimbKLlZPv80dzfNz9aK7WRtKmZYCm3HXomsBkT38kAoloBQkgO5tXVQ0dIitZoN11vzjw8OdA8pSAexUuxj8VUmf19cQOjAIQY/73kVUq1SRQrT7kSWNTSEaYzGcFVvj9fqC/vlNnwkJIAd7L/djcYxUWH0q1jbvd0Rh81GOJvNchIINv2SklTs/30W6v4+hCYuhjKX2dYKdPnUcWb0Li4CP2c+3/NP0/Xx9XXhkuR5z63eAvqeVVRk39TVyVondHJyXN1FR3I3l+uwB8M4fjyKIEnSmcPh7KQigopoksuldxVhGMbasKqv7zYm8/k82zcHgnAnCGIXjmMRlPFBAsf3YRxO4Aq/Yka3Y8PD41nNPDw8EI7jrL8tNUFYeRxbu/8/AchQT6jLtz7ZAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"3 step habits\"\n        title=\"3 step habits\"\n        src=\"/static/39aa210b9e7cd621ef4442a0200f71ce/a6d36/3-step-habits.png\"\n        srcset=\"/static/39aa210b9e7cd621ef4442a0200f71ce/222b7/3-step-habits.png 163w,\n/static/39aa210b9e7cd621ef4442a0200f71ce/ff46a/3-step-habits.png 325w,\n/static/39aa210b9e7cd621ef4442a0200f71ce/a6d36/3-step-habits.png 650w,\n/static/39aa210b9e7cd621ef4442a0200f71ce/e548f/3-step-habits.png 975w,\n/static/39aa210b9e7cd621ef4442a0200f71ce/2bef9/3-step-habits.png 1024w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"what-does-this-have-in-common-with-training-a-neural-network\" style=\"position:relative;\"><a href=\"#what-does-this-have-in-common-with-training-a-neural-network\" aria-label=\"what does this have in common with training a neural network permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What does this have in common with training a neural network?</h2>\n<p>To a single neuron, this is how training looks:</p>\n<ol>\n<li><strong>Cue</strong>: accept inputs from neurons in the previous layer</li>\n<li><strong>Routine</strong>: calculate the sum of the weighted inputs and apply the activation function</li>\n<li><strong>Reward</strong>: during backpropagation, update our input weights according to the loss function</li>\n</ol>\n<p>In fact, the analogy also works at the level of the network as a whole:</p>\n<ol>\n<li><strong>Cue</strong>: transform our input example and input it into the first layer of the network</li>\n<li><strong>Routine</strong>: the network processes the input through its layers to produce a result</li>\n<li><strong>Reward</strong>: calculate how accurate the result is – compared to the labeling of the input example – and backpropagate</li>\n</ol>\n<p>So, from a process perspective there do seem to broad similarities between how we – as humans – form habits, and how we perform supervised machine learning on neural networks.</p>\n<h2 id=\"generalising-habits-system-1-thinking\" style=\"position:relative;\"><a href=\"#generalising-habits-system-1-thinking\" aria-label=\"generalising habits system 1 thinking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generalising habits: System 1 thinking</h2>\n<p>In <a href=\"https://g.co/kgs/vM95DV\">Thinking, Fast and Slow</a>, Daniel Kahneman draws a distinction between <strong>System 1</strong> and <strong>System 2</strong> thinking. The former is responsible for fast, cheap, lossy, inaccurate, and unconsious thought; the latter is responsible for analytical, reasoned, slow, expensive, and conscious thought.</p>\n<p>Habits and habitual behaviour are archetypal examples of System 1 thinking: we can solve extraordinarily complex problems without even noticing it – for example driving a car while our mind wanders a little.</p>\n<h2 id=\"do-neural-networks-behave-like-our-system-1-thinking\" style=\"position:relative;\"><a href=\"#do-neural-networks-behave-like-our-system-1-thinking\" aria-label=\"do neural networks behave like our system 1 thinking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Do neural networks behave like our System 1 thinking?</h2>\n<p>Some hallmarks of System 1 thinking are:</p>\n<ul>\n<li>it’s very quick</li>\n<li>it happens unconsciously</li>\n<li>it’s well-suited to searching for patterns</li>\n<li>it requires training through repetition and reward</li>\n<li>it gives simple answers to complex questions</li>\n<li>it’s prone to cognitive errors</li>\n</ul>\n<p>The similarities between these attributes and the behaviour of trained neural networks are striking.</p>\n<p>Admittedly, I cherry-picked these attributes to <em>some</em> degree but – apart from certain human-centric cognitive errors that we suffer from – it’s hard to find a single characteristic of System 1 thinking which isn’t also evident in artificial neural networks.</p>\n<h2 id=\"what-does-this-mean-for-machine-cognition\" style=\"position:relative;\"><a href=\"#what-does-this-mean-for-machine-cognition\" aria-label=\"what does this mean for machine cognition permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What does this mean for machine cognition?</h2>\n<p>Kahneman doesn’t imply that his System 1/2 abstraction represents an actual underlying psychophysiological distinction: both systems are powered by the same underlying hardware – neurons – after all. On the other hand, <a href=\"https://theeconreview.com/2017/01/13/what-neuroscience-has-to-say-about-decision-making/\">fMRI experiments</a> show that System 2 is associated with increased frontal and parietal cortex activity, so perhaps there really is a structural difference between the two systems.</p>\n<p>Whether there is a physiological distinction between the systems or not, it seems clear that there is a wide spectrum along which cognition can happen – with System 1 at one end and System 2 at the other.</p>\n<p>As described above, supervised machine learning looks to be at the same end of that spectrum as System 1: from both a process and behaviour perspective.</p>\n<p>The two clearest conclusions from this are:</p>\n<h2 id=\"1-artificial-general-intelligence-wont-come-from-supervised-machine-learning\" style=\"position:relative;\"><a href=\"#1-artificial-general-intelligence-wont-come-from-supervised-machine-learning\" aria-label=\"1 artificial general intelligence wont come from supervised machine learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Artificial General Intelligence won’t come from supervised machine learning</h2>\n<p>AGI is a dream or a nightmare depending on whom you talk to.</p>\n<p>Either way, although supervised neural networks can take you deep into the <a href=\"https://en.wikipedia.org/wiki/Uncanny_valley\">uncanny valley</a>, it seems like there will always be <strong>something</strong> missing if they just get better and better, faster and faster, at System 1-like responses.</p>\n<h2 id=\"2-we-shouldnt-expect-neural-networks-to-be-able-to-explain-their-decisions\" style=\"position:relative;\"><a href=\"#2-we-shouldnt-expect-neural-networks-to-be-able-to-explain-their-decisions\" aria-label=\"2 we shouldnt expect neural networks to be able to explain their decisions permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. We shouldn’t expect neural networks to be able to explain their decisions</h2>\n<p>Yes, we could dump out the weights of all the neurons to fully “explain” a neural network. However, in the same way as performing that procedure on a human wouldn’t “explain” their personality, we don’t gain real insight from merely understanding the hardware.</p>\n<p>There has been <a href=\"https://arxiv.org/pdf/1612.04757v1.pdf\">some progress</a> on generating commentary alongside a particular decision but its domain is very limited. Many leading AI researchers have shifted in recent years to focussing on the statistical outcomes of the AI and optimising there – rather than demanding a wholesale account of each individual decision.</p>\n<p>Neural networks <strong>just don’t work</strong> in a logical, considered way which can be dissected and analysed afterwards. There is no rational answer to be explained so it would behove us to stop asking the question.</p>\n\n      <div class=\"footnotes\">\n        <hr/>\n        <ol >\n    \n    <li class=\"footnote-list-item\" id=\"fn-1\" >\n          \n        <p class=\"footnote-paragraph\" style=\"display:inline; \">A more complete version of this would include careful selection of training, dev, and test sets, along with changes to the network architecture, hyperparameters, regularisation, … However, at its core, this simplified version is where the kernel of automated learning happens.</p>\n      <a href=\"#fnref-1\" class=\"footnote-backref\" style=\"display:inline;\">\n        ↟\n      </a>\n    \n      </li>\n      \n    \n\n    <li class=\"footnote-list-item\" id=\"fn-2\" >\n          \n        <p class=\"footnote-paragraph\" style=\"display:inline; \">I use Duhigg’s formulation here rather than <a href=\"https://jamesclear.com/habit-triggers\">James Clear’s</a> because I don’t think splitting <strong>Routine</strong> into <strong>Cue</strong> and <strong>Response</strong> makes sense for neural networks.</p>\n      <a href=\"#fnref-2\" class=\"footnote-backref\" style=\"display:inline;\">\n        ↟\n      </a>\n    \n      </li>\n      \n    \n\n    <li class=\"footnote-list-item\" id=\"fn-3\" >\n          \n        <p class=\"footnote-paragraph\" style=\"display:inline; \">Cover photo by <a href=\"https://unsplash.com/@alinnnaaaa?utm_source=unsplash&#x26;utm_medium=referral&#x26;utm_content=creditCopyText\">Alina Grubnyak</a> on <a href=\"https://unsplash.com/?utm_source=unsplash&#x26;utm_medium=referral&#x26;utm_content=creditCopyText\">Unsplash</a></p>\n      <a href=\"#fnref-3\" class=\"footnote-backref\" style=\"display:inline;\">\n        ↟\n      </a>\n    \n      </li>\n      \n    </ol></div>","frontmatter":{"title":"Is training a neural network like forming a habit?","description":"There are striking similarities between the training of a neural network and the way that we – as humans – form habits. How should that inform our thinking about the potential for this type of artificial intelligence, and what we should and shouldn't be asking of it?","keywords":["tech","ai","artificial intelligence","machine learning","habits","neural networks","back-propagation","system 1","system 2"],"coverImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAIBAwT/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB3q8WMWB//8QAFxAAAwEAAAAAAAAAAAAAAAAAAAExIf/aAAgBAQABBQLBVEdEf//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAEQESD/2gAIAQEABj8CmCv/xAAaEAEBAQEAAwAAAAAAAAAAAAABEQAxUWGR/9oACAEBAAE/IWJR6y8dE4/MtSYKHMIb/9oADAMBAAIAAwAAABDzH//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQADAQEBAAAAAAAAAAAAAAEAETEhQXH/2gAIAQEAAT8QQh4UZ0L1hXpGlwpTkBXkATzYeSS71PkAQMn/2Q==","aspectRatio":1.5015015015015014,"src":"/static/60409e3b1964a3d2a8389370f11366dc/a41d1/neural-mesh.jpg","srcSet":"/static/60409e3b1964a3d2a8389370f11366dc/0f3a1/neural-mesh.jpg 500w,\n/static/60409e3b1964a3d2a8389370f11366dc/a7715/neural-mesh.jpg 1000w,\n/static/60409e3b1964a3d2a8389370f11366dc/a41d1/neural-mesh.jpg 2000w,\n/static/60409e3b1964a3d2a8389370f11366dc/b2131/neural-mesh.jpg 3000w,\n/static/60409e3b1964a3d2a8389370f11366dc/70963/neural-mesh.jpg 3456w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}}},"pageContext":{"slug":"/habit-formation-as-analogy-for-machine-learning/"}}}